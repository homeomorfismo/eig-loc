\documentclass[12pt]{amsart}

%%% Packages
\usepackage[english]{babel}
\usepackage{amsmath, amsfonts, amssymb, amsthm, mathtools, mathrsfs, stmaryrd} % Standard math packages
\usepackage{hyperref, orcidlink} % Hyperlinks and ORCID
\usepackage{pgfplots, pgfplotstable, siunitx} % Graphics
\usepackage[Tol]{colorblind} % Colorblind-friendly colors
\usepackage{float, graphicx, caption, subcaption, etoolbox, anyfontsize, multirow} % Improved interfaces
\usepackage{tikz} % TiKz

%%%%%% Packages configuration
\sisetup{
    round-mode = places,
    round-precision = 5,
}

\pgfplotsset{compat=1.18}

% Colorblind-friendly configuration for pgfplots
\pgfplotscreateplotcyclelist{cblist}{
    {T-Q-B1, thick, mark=o},
    {T-Q-B2, thick, mark=+},
    {T-Q-B3, thick, mark=otimes},
    {T-Q-B4, thick, mark=triangle*},
    {T-Q-B5, thick, mark=diamond*},
    {T-Q-B6, thick, mark=square},
    {T-Q-HC0,thick, mark=x}
}

\usetikzlibrary{shapes.geometric, calc, cd}

\newcommand{\orcidID}[1]{\textsuperscript{\orcidlink{#1}}}

%%%%%%% AMS Article configuration
\title[Landscape-driven AFEM]{Landscape-driven Adaptive Finite Element Methods for the Laplacian Eigenvalue Problem}

\author{
    Jeffrey Ovall \and % \inst{1} \and
    Gabriel Pinochet-Soto \orcidID{0009-0008-0626-8194} % \inst{1}
}

\address{
    Fariborz Maseeh Department of Mathematics and Statistics, Portland State University, Portland, OR, USA % \inst{1}
}

\thanks{
    \textbf{Acknowledgments.} The authors would like to thank
    % TODO: Add acknowledgments
}

%%% Document
\begin{document}

\maketitle

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Setting}

Consider the source problem
\begin{subequations}
\begin{align}
    \label{eq:source}
    - \Delta u  & = 1, \text{ in } \Omega, \\
    u & = 0, \text{ on } \partial \Omega,
\end{align}
\end{subequations}
and the eigenvalue problem
\begin{subequations}
\begin{align}
    \label{eq:eigenvalue}
    - \Delta \psi & = \lambda \psi, \text{ in } \Omega, \\
    \psi & = 0, \text{ on } \partial \Omega.
\end{align}
\end{subequations}

We write the weak formulations of the above problems.
Define the bilinear forms
\begin{subequations}
\begin{align}
    a: & \mathring{H}^1(\Omega) \times \mathring{H}^1(\Omega) \to \mathbb{R}, &
    a(u, v) & = \int_{\Omega} \nabla u \cdot \nabla v \, \mathrm{d}x, \\
    b: & \mathring{H}^1(\Omega) \times \mathring{H}^1(\Omega) \to \mathbb{R}, &
    b(u, v) & = \int_{\Omega} u v \, \mathrm{d}x,
\end{align}
and the linear form
\begin{align}
    \ell: & \mathring{H}^1(\Omega) \to \mathbb{R}, &
    \ell (v) & = \int_{\Omega} v \, \mathrm{d}x.
\end{align}
\end{subequations}

Then, the weak formulation of the source problem is to find \(u \in \mathring{H}^1(\Omega)\) such that
\begin{equation}
    \label{eq:source_weak}
    a(u, v) = \ell(v), \quad \forall v \in \mathring{H}^1(\Omega),
\end{equation}
for all test functions \(v \in \mathring{H}^1(\Omega)\).
Similarly, the weak formulation of the eigenvalue problem is to find \((\lambda, \psi) \in \mathbb{C} \times \mathring{H}^1(\Omega)\) such that
\begin{equation}
    \label{eq:eigenvalue_weak}
    a(\psi, v) = \lambda b(\psi, v), \quad \forall v \in \mathring{H}^1(\Omega),
\end{equation}
for all test functions \(v \in \mathring{H}^1(\Omega)\).

We now consider the finite element discretization of the above problems.
Let \(\Omega_h\) be an \(h\)-parametrized family of triangulations of \(\Omega\),
and let \(V_h\) be an \(\mathring{H}^1\)-conforming finite element space on \(\Omega_h\).
Consider the restriction of the bilinear forms \(a\) and \(b\) and the linear form \(\ell\) to \(V_h\);
we denote these restrictions by \(a_h\), \(b_h\), and \(\ell_h\), respectively.
Then, the finite element discretization of the source problem is to find \(u_h \in V_h\) such that
\begin{equation}
    \label{eq:source_fem}
    a_h(u_h, v_h) = \ell_h(v_h), \quad \forall v_h \in V_h,
\end{equation}
for all test functions \(v \in V_h\).
Similarly, the finite element discretization of the eigenvalue problem is to find \((\lambda_h, \psi_h) \in \mathbb{C} \times V_h\) such that
\begin{equation}
    \label{eq:eigenvalue_fem}
    a_h(\psi_h, v_h) = \lambda_h b_h(\psi_h, v_h), \quad \forall v_h \in V_h,
\end{equation}
for all test functions \(v \in V_h\).

From the standard spectral theory of elliptic operators, we know that the eigenvalues of a self-adjoint elliptic operator are real and bounded below. [REFERENCE NEEDED]
Moreover, the eigenfunctions corresponding to distinct eigenvalues are orthogonal.
We pick these vectors to be orthonormal with respect to the inner product in \(L^2(\Omega)\), i.e.,
\begin{equation}
    \label{eq:orthonormality}
    (\psi_i, \psi_j) = \int_{\Omega} \psi_i \psi_j \, \mathrm{d}x = \delta_{ij}.
\end{equation}
Similarly, we pick the discretized eigenfunctions \(\{\psi_{h, i}\}_{i=0}^{N(h)}\) to be orthonormal with respect to the inner product in \(L^2(\Omega)\), i.e.,
\begin{equation}
    \label{eq:orthonormality_fem}
    (\psi_{h, i}, \psi_{h, j}) = \int_{\Omega} \psi_{h, i} \psi_{h, j} \, \mathrm{d}x = \delta_{ij}.
\end{equation}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Error Estimators}

We will define a local residual-based error estimator for the source problem. [REFERENCE NEEDED]
Given \(K \in \Omega_h\), let \(u_K\) be the solution of the source problem restricted to \(K\).
Denote by \(\llbracket u \rrbracket_F\) the jump of \(u\) across the face \(F\).
Then, the local residual-based error estimator is given by
\begin{equation}
    \label{eq:source_error_estimator}
    \eta_K^2(u) =
    \eta_K^2 = h_K^2 \left\lVert -\Delta u_K - 1 \right\rVert_{L^2(K)}^2
         + \frac{h_K}{2} \left\lVert \llbracket \partial_\nu u_K \rrbracket_{\partial K} \right\rVert_{L^2(\partial K)}^2,
\end{equation}
where \(h_K\) is the diameter of \(K\).
The global error estimator is then given by
\begin{equation}
    \label{eq:source_global_error_estimator}
    % \eta^2 = \sum_{K \in \Omega_h} \eta_K^2.
    \eta = \max_{K \in \Omega_h} \eta_K.
\end{equation}
Another possible global error estimator is given by
\begin{equation}
    \label{eq:source_global_error_estimator_alt}
    \eta^2_{\ell^2}(u) =
    \sum_{K \in \Omega_h} \eta_K^2.
\end{equation}

For verification purposes, we also define a local residual-based error estimator for the eigenvalue problem.
Given \(K \in \Omega_h\), let \(\psi_K\) be the solution of the eigenvalue problem restricted to \(K\).
Then, the local residual-based error estimator is given by
\begin{equation}
    \label{eq:eigenvalue_error_estimator}
    \eta_K^2(\psi, \lambda) =
    h_K^2 \left\lVert -\Delta \psi_K - \lambda \psi_K \right\rVert_{L^2(K)}^2
        + \frac{h_K}{2} \left\lVert \llbracket \partial_\nu \psi_K \rrbracket_{\partial K} \right\rVert_{L^2(\partial K)}^2.
\end{equation}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{\(h\)-Adaptive Algorithm}

We will now describe the \(h\)-adaptive algorithm for the source problem.
A high-level description of the algorithm consists of a loop over the following steps:
\textbf{SOLVE}, \textbf{ESTIMATE}, \textbf{MARK}, and \textbf{REFINE}.

Consider the triangulation \(\Omega_{(n)}\) at iteration \(n\).
We say that an element \(K \in \Omega_{(n)}\) is marked for refinement if \(\eta_K > \theta \eta\), where \(\theta\) is a user-defined threshold, and \(\eta\) is the global error estimator defined in \eqref{eq:source_global_error_estimator}.
We define a new triangulation \(\Omega_{(n+1)}\) at iteration \(n+1\) by bisecting all elements \(K \in \Omega_{(n)}\) that are marked for refinement.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% TODO: Check repeated labels
\section{Numerical Experiments}

% TODO: Add more details...
%   ORDER = 2
%   IS_COMPLEX = True
%   MAX_ITER = 45
%   THETA = 0.75
%   
%   # FEAST parameters
%   NSPAN = 10
%   NPTS = 10
%   CHECKS = False
%   RADIUS = 10.0
%   CENTER = 50 * np.pi**2
%   
%   FEAST_PARAMS = {
%       "hermitian": True,
%       "stop_tol": 1.0e-10,
%       "cut_tol": 1.0e-10,
%       "eta_tol": 1e-10,
%       "nrestarts": 5,
%       "niterations": 100,
%   }

We consider an L-shaped domain \(\Omega = [-1, 1]^2 \setminus [0, 1] \times [0, 1]\).
We set the threshold for the \emph{greedy marking} strategy to \(\theta = 0.75\).
The maximum diameter for the elements defined in the initial stage is \(h_{\max} = 0.3\).
We set the maximum number of iterations to \(n_{\operatorname{MaxIter}} = 45\).

Concerning the eigensolver, we decide to use the FEAST algorithm [REFERENCE NEEDED] to compute the eigenvalues.
We set the maximum number of desired eigenvalues to \(d = 10\), and the number of quadrature points to \(n_{\operatorname{pts}} = 10\).
% We set a circular region of interest centered at \(\lambda = 50\pi^2\) with radius \(r = 10\).
% The stopping tolerance for the eigensolver is set to \(\varepsilon_{\operatorname{stop}} = 1.0 \times 10^{-10}\),
% the cut-off tolerance is set to \(\varepsilon_{\operatorname{cut}} = 1.0 \times 10^{-10}\),
% and the error tolerance is set to \(\varepsilon_{\operatorname{err}} = 1.0 \times 10^{-10}\).
% We cap the number of restarts to \(n_{\operatorname{restarts}} = 5\), and the maximum number of iterations to \(n_{\operatorname{iter}} = 100\).

% (Real part of) eigenvalues of the Laplacian on the unit square with Dirichlet boundary conditions
% close to the interval \([50\pi^2 - 10, 50\pi^2 + 10]\).
\begin{figure}[htpb]
    \centering
    \begin{tikzpicture}
        \begin{semilogxaxis}[
            xlabel={Number of degrees of freedom},
            ylabel={Eigenvalues},
            legend pos=south west,
            grid=major,
            cycle list name=cblist,
            xmin=150,
            xmax=120000,
            clip=false,
            forget plot style={black!75, dotted},
            legend entries={
                {\(\lambda_{i,n}\)},
                {\(\lambda_{i,N}\)},
            },
        ]
        % Add data
        \addplot+ [only marks] table[x=Ndofs, y=Evals, col sep=comma] {eigenvalues_landscape.csv};

        % Add horizontal lines
        \addplot[forget plot, mark=none, samples=20] coordinates { (159, 485.7222447031804)  (120000, 485.7222447031804)};
        \addplot[forget plot, mark=none, samples=20] coordinates { (159, 490.1641386920197)  (120000, 490.1641386920197)};
        \addplot[forget plot, mark=none, samples=20] coordinates { (159, 493.48390547991005) (120000, 493.48390547991005)};
        \addplot[forget plot, mark=none, samples=20] coordinates { (159, 493.48401556963995) (120000, 493.48401556963995)};
        \addplot[forget plot, mark=none, samples=20] coordinates { (159, 493.48452662344147) (120000, 493.48452662344147)};
        \addplot[forget plot, mark=none, samples=20] coordinates { (159, 499.24505256207357) (120000, 499.24505256207357)};
        \addplot[forget plot, mark=none, samples=20] coordinates { (159, 502.3049699107303)  (120000, 502.3049699107303)};

        % Add markers
        \node [pin=right:{\(\lambda_{0}\)}] at (axis cs: 100000, 485.7222447031804) {};
        \node [pin=right:{\(\lambda_{1}\)}] at (axis cs: 100000, 490.1641386920197) {};
        \node [pin=right:{\(\lambda_{2, 3, 4}\)}] at (axis cs: 100000, 493.48390547991005) {};
        \node [pin=right:{\(\lambda_{5}\)}] at (axis cs: 100000, 499.24505256207357) {};
        \node [pin=right:{\(\lambda_{6}\)}] at (axis cs: 100000, 502.3049699107303) {};

        \end{semilogxaxis}
    \end{tikzpicture}
    \caption{
        Eigenvalues inside the interval \([50\pi^2 - 10, 50\pi^2 + 10]\), under \emph{landscape-driven refinement}.
        \(\lambda_{i,n}\) denotes the \(i\)-th eigenvalue at \(n\)-th iteration, and \(\lambda_{i,N}\) denotes the \(i\)-th eigenvalue at the final iteration.
        Here, \(\lambda_{0,N} = 485.7222447031804\), \(\lambda_{1,N} = 490.1641386920197\), \(\lambda_{2,N} \approx \lambda_{3,N} \approx \lambda_{4,N} \approx 50\pi^2\), \(\lambda_{5,N} = 499.24505256207357\), and \(\lambda_{6,N} = 502.3049699107303\).
    }
    \label{fig:eigenvalues-landscape}
\end{figure}


% Evolution of the error estimator \(\eta := \eta(u)\) as a function of the number of degrees of freedom.
\begin{figure}[htpb]
    \centering
    \begin{tikzpicture}
        \begin{loglogaxis}[
            xlabel={Number of degrees of freedom},
            ylabel={Error estimator},
            legend entries={\(\eta\)},
            legend pos=south west,
            cycle list name=cblist,
            grid=major,
            forget plot style={black!75, dotted},
        ]
        \addplot+ table[x=Ndofs, y={Eta}, col sep=comma] {error_estimator_landscape.csv};
        \addplot [forget plot] table [
            col sep=comma,
            x=Ndofs,
            y={create col/linear regression={
                y=Eta
                % variance list={1000,800,600,500,400,200,100}
                }
            }] {error_estimator_landscape.csv}
            coordinate [pos=0.8] (A)
            coordinate [pos=0.9] (B);
        \xdef\slope{\pgfplotstableregressiona}
        \draw (A) -| (B) node[pos=0.75, anchor=west] {\(\pgfmathprintnumber{\slope}\)};
        \end{loglogaxis}
    \end{tikzpicture}
    \caption{Error estimator \(\eta := \eta(u)\) (based on the landscape function \(u\)) as a function of the number of degrees of freedom.}
    \label{fig:ee-landscape}
\end{figure}

% Monitoring the behavior of the error estimator \(\eta_i := \eta(\psi_i, \lambda_i)\) as a function of the number of degrees of freedom.
\begin{figure}[htpb]
    \centering
    \begin{tikzpicture}
        \begin{loglogaxis}[
            xlabel={Number of degrees of freedom},
            ylabel={Error estimator},
            legend entries={{\(\eta_0\)},
                            {\(\eta_1\)},
                            {\(\eta_2\)},
                            {\(\eta_3\)},
                            {\(\eta_4\)},
                            {\(\eta_5\)},
                            {\(\eta_6\)}},
            legend pos=south west,
            cycle list name=cblist,
            grid=major,
            forget plot style={black!75, dotted},
        ]
        % \addplot table[x=Ndofs, y={Eta}, col sep=comma] {error_estimator.csv};
        \addplot+ table[x=Ndofs, y=Eta Eigenvalue 0, col sep=comma] {error_estimator_landscape.csv};
        \addplot+ table[x=Ndofs, y=Eta Eigenvalue 1, col sep=comma] {error_estimator_landscape.csv};
        \addplot+ table[x=Ndofs, y=Eta Eigenvalue 2, col sep=comma] {error_estimator_landscape.csv};
        \addplot+ table[x=Ndofs, y=Eta Eigenvalue 3, col sep=comma] {error_estimator_landscape.csv};
        \addplot+ table[x=Ndofs, y=Eta Eigenvalue 4, col sep=comma] {error_estimator_landscape.csv};
        \addplot+ table[x=Ndofs, y=Eta Eigenvalue 5, col sep=comma] {error_estimator_landscape.csv};
        \addplot+ table[x=Ndofs, y=Eta Eigenvalue 6, col sep=comma] {error_estimator_landscape.csv};

        \addplot [forget plot] table[
            x=Ndofs,
            y={create col/linear regression={y=Eta Eigenvalue 0}},
            col sep=comma] {error_estimator_landscape.csv}
            coordinate [pos=0.8] (A)
            coordinate [pos=0.9] (B);
        \xdef\slope{\pgfplotstableregressiona}
        \draw (A) -| (B) node[pos=0.75, anchor=west] {\(\pgfmathprintnumber{\slope}\)};

        \addplot [forget plot] table[
            x=Ndofs,
            y={create col/linear regression={y=Eta Eigenvalue 4}},
            col sep=comma] {error_estimator_landscape.csv}
            coordinate [pos=0.6] (A)
            coordinate [pos=0.7] (B);
        \xdef\slopee{\pgfplotstableregressiona}
        \draw (A) -| (B) node[pos=0.75, anchor=west] {\(\pgfmathprintnumber{\slopee}\)};

        \end{loglogaxis}
    \end{tikzpicture}
    \caption{Error estimator \(\eta_i := \eta(\psi_i, \lambda_i)\) as a function of the number of degrees of freedom, under landscape-driven refinement.}
    \label{fig:ev-ee-landscape}
\end{figure}


% Visualizing the error of the eigenvalues, after landscape-driven refinement.
\begin{figure}[htpb]
    \centering
    \begin{tikzpicture}
        \begin{loglogaxis}[
            forget plot style={black!75, dotted},
            xlabel={Number of degrees of freedom},
            ylabel={\(\left\lvert \lambda_{i,n} - \lambda_{i,N} \right\rvert\)},
            cycle list name=cblist,
            legend pos=south west,
            grid=major,
            % xmin=150, xmax=257000,
            legend entries={\(\operatorname{err}_0\),
                            \(\operatorname{err}_1\),
                            \(\operatorname{err}_2\),
                            \(\operatorname{err}_3\),
                            \(\operatorname{err}_4\),
                            \(\operatorname{err}_5\),
                            \(\operatorname{err}_6\)},
        ]
        \addplot+ table[x=Ndofs, y=Error Eigenvalue 0, col sep=comma] {error_eigenvalues_landscape.csv};
        \addplot+ table[x=Ndofs, y=Error Eigenvalue 1, col sep=comma] {error_eigenvalues_landscape.csv};
        \addplot+ table[x=Ndofs, y=Error Eigenvalue 2, col sep=comma] {error_eigenvalues_landscape.csv};
        \addplot+ table[x=Ndofs, y=Error Eigenvalue 3, col sep=comma] {error_eigenvalues_landscape.csv};
        \addplot+ table[x=Ndofs, y=Error Eigenvalue 4, col sep=comma] {error_eigenvalues_landscape.csv};
        \addplot+ table[x=Ndofs, y=Error Eigenvalue 5, col sep=comma] {error_eigenvalues_landscape.csv};
        \addplot+ table[x=Ndofs, y=Error Eigenvalue 6, col sep=comma] {error_eigenvalues_landscape.csv};


        \addplot [forget plot] table[
            x=Ndofs,
            y={create col/linear regression={y=Error
            Eigenvalue 0}},
            col sep=comma] {error_eigenvalues_landscape.csv}
            coordinate [pos=0.8] (A)
            coordinate [pos=0.9] (B);
        \xdef\slope{\pgfplotstableregressiona}
        \draw (A) -| (B) node[pos=0.75, anchor=west] {\(\pgfmathprintnumber{\slope}\)};
        \end{loglogaxis}
    \end{tikzpicture}
    \caption{Error \(\operatorname{err}_i := \left\lvert \lambda_{i,n} - \lambda_{i,N} \right\rvert\) as a function of the number of degrees of freedom, under landscape-driven refinement.}
    \label{fig:error-ev-landscape}
\end{figure}


% Table of convergence ratios, under landscape-driven refinement.
% TODO: Fix scientific notation!
% \begin{table}[h!]
%     \begin{center}
%         \label{tab:convergence}
%         \pgfplotstabletypeset[
%             col sep=comma,
%             columns={Ndofs, Ratio 0, Ratio 1, Ratio 2, Ratio 3, Ratio 4, Ratio 5, Ratio 6},
%             columns/Ndofs/.style={
%                 column name={\#\textsc{DoFs}},
%                 int detect,
%                 column type={c}
%             },
%             columns/Ratio 0/.style={
%                 column name={\(\operatorname{ord}_0\)},
%                 precision=2,
%                 fixed zerofill,
%                 column type={c}
%             },
%             columns/Ratio 1/.style={
%                 column name={\(\operatorname{ord}_1\)},
%                 precision=2,
%                 fixed zerofill,
%                 column type={c}
%             },
%             columns/Ratio 2/.style={
%                 column name={\(\operatorname{ord}_2\)},
%                 precision=2,
%                 fixed zerofill,
%                 column type={c}
%             },
%             columns/Ratio 3/.style={
%                 column name={\(\operatorname{ord}_3\)},
%                 precision=2,
%                 fixed zerofill,
%                 column type={c}
%             },
%             columns/Ratio 4/.style={
%                 column name={\(\operatorname{ord}_4\)},
%                 precision=2,
%                 fixed zerofill,
%                 column type={c}
%             },
%             columns/Ratio 5/.style={
%                 column name={\(\operatorname{ord}_5\)},
%                 precision=2,
%                 fixed zerofill,
%                 column type={c}
%             },
%             columns/Ratio 6/.style={
%                 column name={\(\operatorname{ord}_6\)},
%                 precision=2,
%                 fixed zerofill,
%                 column type={c}
%             },
%             ]{ratio_ev_landscape.csv}
%         \caption{
%             Order of convergence of the eigenvalues.
%             Here, \(\operatorname{err}_i \approx C_i N(h)^{-\operatorname{ord}_i}\).
%         }
%     \end{center}
% \end{table}

% Table of convergence (decay) ratio for the landscape error estimator.
% \begin{table}
%     \begin{center}
%         \label{tab:error}
%         \pgfplotstabletypeset[
%             col sep=comma,
%             columns={Ndofs, Ratio},
%             columns/Ndofs/.style={
%                 column name={\#\textsc{DoFs}},
%                 int detect,
%                 column type={c}
%             },
%             columns/Ratio/.style={
%                 column name={\(\operatorname{ord}\)},
%                 precision=2,
%                 fixed zerofill,
%                 column type={c}
%             },
%             ]{ratio_ev_landscape.csv}
%         \caption{
%             Order of convergence of the landscape error estimator.
%             Here, \(\eta \approx C N(h)^{-\operatorname{ord}}\).
%         }
%     \end{center}
% \end{table}

% Pictures of the mesh during the adaptive algorithm, under landscape-driven refinement.
\begin{figure}[htbp]
    \begin{subfigure}[t]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{mesh/mesh_landscape_0.png}
        \caption{Initial mesh.}
        \label{fig:mesh_0}
    \end{subfigure}
    \begin{subfigure}[t]{0.49\textwidth}
        \centering
        % \includegraphics[width=0.5\textwidth, trim={30cm 60cm 30cm 60cm}, clip]{mesh_10.png}
        \includegraphics[width=\textwidth]{mesh/mesh_landscape_10.png}
        \caption{Mesh after ten iterations.}
        \label{fig:mesh_10}
    \end{subfigure}
    \vspace{1cm}
    \begin{subfigure}[t]{0.49\textwidth}
        \centering
        % \includegraphics[width=0.5\textwidth, trim={30cm 60cm 30cm 60cm}, clip]{mesh_20.png}
        \includegraphics[width=\textwidth]{mesh/mesh_landscape_20.png}
        \caption{Mesh after twenty iterations.}
        \label{fig:mesh_20}
    \end{subfigure}
    \begin{subfigure}[t]{0.49\textwidth}
        \centering
        % \includegraphics[width=0.5\textwidth, trim={30cm 60cm 30cm 60cm}, clip]{mesh_30.png}
        \includegraphics[width=\textwidth]{mesh/mesh_landscape_30.png}
        \caption{Mesh after thirty iterations.}
        \label{fig:mesh_30}
    \end{subfigure}
    \vspace{1cm}
    \begin{subfigure}[t]{0.49\textwidth}
        \centering
        % \includegraphics[width=0.5\textwidth, trim={30cm 60cm 30cm 60cm}, clip]{mesh_40.png}
        \includegraphics[width=\textwidth]{mesh/mesh_landscape_40.png}
        \caption{Mesh after forty iterations.}
        \label{fig:mesh_40}
    \end{subfigure}
    \begin{subfigure}[t]{0.49\textwidth}
        \centering
        % \includegraphics[width=0.5\textwidth, trim={30cm 60cm 30cm 60cm}, clip]{mesh_49.png}
        \includegraphics[width=\textwidth]{mesh/mesh_landscape_44.png}
        \caption{Final mesh.}
        \label{fig:mesh_50}
    \end{subfigure}
    \caption{Meshes during the adaptive algorithm, under landscape-driven refinement.}
\end{figure}


% Eigenvalues, after landscape-driven refinement. Real part.
\begin{figure}[htbp]
    \begin{subfigure}[t]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{landscape/ev0}
        \caption{Eigenfunction \(\psi_0\).}
        \label{fig:eigenfunction_0}
    \end{subfigure}
    \begin{subfigure}[t]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{landscape/ev1}
        \caption{Eigenfunction \(\psi_1\).}
        \label{fig:eigenfunction_1}
    \end{subfigure}
    \vspace{1cm}
    \begin{subfigure}[t]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{landscape/ev2}
        \caption{Eigenfunction \(\psi_2\).}
        \label{fig:eigenfunction_2}
    \end{subfigure}
    \begin{subfigure}[t]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{landscape/ev3}
        \caption{Eigenfunction \(\psi_3\).}
        \label{fig:eigenfunction_3}
    \end{subfigure}
    \vspace{1cm}
    \begin{subfigure}[t]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{landscape/ev4}
        \caption{Eigenfunction \(\psi_4\).}
        \label{fig:eigenfunction_4}
    \end{subfigure}
    \begin{subfigure}[t]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{landscape/ev5}
        \caption{Eigenfunction \(\psi_5\).}
        \label{fig:eigenfunction_5}
    \end{subfigure}
    \vspace{1cm}
    \begin{subfigure}[t]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{landscape/ev6}
        \caption{Eigenfunction \(\psi_6\).}
        \label{fig:eigenfunction_6}
    \end{subfigure}
    \caption{Real part of the eigenfunctions \(\psi_i\) for \(i = 0, \ldots, 6\) at the final iteration of the adaptive algorithm, under landscape-driven refinement.}
\end{figure}


% Eigenvalues, after landscape-driven refinement. Absolute value.
\begin{figure}[htbp]
    \begin{subfigure}[t]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{landscape/abs_ev0}
        \caption{Eigenfunction \(\psi_0\).}
        \label{fig:eigenfunction_0}
    \end{subfigure}
    \begin{subfigure}[t]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{landscape/abs_ev1}
        \caption{Eigenfunction \(\psi_1\).}
        \label{fig:eigenfunction_1}
    \end{subfigure}
    \vspace{1cm}
    \begin{subfigure}[t]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{landscape/abs_ev2}
        \caption{Eigenfunction \(\psi_2\).}
        \label{fig:eigenfunction_2}
    \end{subfigure}
    \begin{subfigure}[t]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{landscape/abs_ev3}
        \caption{Eigenfunction \(\psi_3\).}
        \label{fig:eigenfunction_3}
    \end{subfigure}
    \vspace{1cm}
    \begin{subfigure}[t]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{landscape/abs_ev4}
        \caption{Eigenfunction \(\psi_4\).}
        \label{fig:eigenfunction_4}
    \end{subfigure}
    \begin{subfigure}[t]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{landscape/abs_ev5}
        \caption{Eigenfunction \(\psi_5\).}
        \label{fig:eigenfunction_5}
    \end{subfigure}
    \vspace{1cm}
    \begin{subfigure}[t]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{landscape/abs_ev6}
        \caption{Eigenfunction \(\psi_6\).}
        \label{fig:eigenfunction_6}
    \end{subfigure}
    \caption{Norm of the eigenfunctions \(\psi_i\) for \(i = 0, \ldots, 6\) at the final iteration of the adaptive algorithm, under landscape-driven refinement.}
\end{figure}


%%% eigenvalue-driven refinement

% (Real part of) eigenvalues of the Laplacian on the unit square with Dirichlet boundary conditions
% close to the interval \([50\pi^2 - 10, 50\pi^2 + 10]\).
\begin{figure}[htbp]
    \centering
    \begin{tikzpicture}
        \begin{semilogxaxis}[
            xlabel={Number of degrees of freedom},
            ylabel={Eigenvalues},
            forget plot style={black!75, dotted},
            legend pos=south west,
            grid=major,
            cycle list name=cblist,
            xmin=150,
            xmax=120000,
            clip=false,
            legend entries={
                {\(\lambda_{i,n}\)},
                {\(\lambda_{i,N}\)},
            },
        ]
        % Add data
        \addplot+ [only marks] table[x=Ndofs, y=Evals, col sep=comma] {eigenvalues_eigenvalues.csv};

        % Add horizontal lines
        \addplot[forget plot, mark=none, samples=20] coordinates { (159, 485.7222447031804)  (120000, 485.7222447031804)};
        \addplot[forget plot, mark=none, samples=20] coordinates { (159, 490.1641386920197)  (120000, 490.1641386920197)};
        \addplot[forget plot, mark=none, samples=20] coordinates { (159, 493.48390547991005) (120000, 493.48390547991005)};
        \addplot[forget plot, mark=none, samples=20] coordinates { (159, 493.48401556963995) (120000, 493.48401556963995)};
        \addplot[forget plot, mark=none, samples=20] coordinates { (159, 493.48452662344147) (120000, 493.48452662344147)};
        \addplot[forget plot, mark=none, samples=20] coordinates { (159, 499.24505256207357) (120000, 499.24505256207357)};
        \addplot[forget plot, mark=none, samples=20] coordinates { (159, 502.3049699107303)  (120000, 502.3049699107303)};

        % Add markers
        \node [pin=right:{\(\lambda_{0}\)}] at (axis cs: 100000, 485.7222447031804) {};
        \node [pin=right:{\(\lambda_{1}\)}] at (axis cs: 100000, 490.1641386920197) {};
        \node [pin=right:{\(\lambda_{2, 3, 4}\)}] at (axis cs: 100000, 493.48390547991005) {};
        \node [pin=right:{\(\lambda_{5}\)}] at (axis cs: 100000, 499.24505256207357) {};
        \node [pin=right:{\(\lambda_{6}\)}] at (axis cs: 100000, 502.3049699107303) {};

        \end{semilogxaxis}
    \end{tikzpicture}
    \caption{
        Eigenvalues inside the interval \([50\pi^2 - 10, 50\pi^2 + 10]\), under \emph{eigenvalue-driven refinement}.
        \(\lambda_{i,n}\) denotes the \(i\)-th eigenvalue at \(n\)-th iteration, and \(\lambda_{i,N}\) denotes the \(i\)-th eigenvalue at the final iteration.
        Here, \(\lambda_{0,N} = 485.7222447031804\), \(\lambda_{1,N} = 490.1641386920197\), \(\lambda_{2,N} \approx \lambda_{3,N} \approx \lambda_{4,N} \approx 50\pi^2\), \(\lambda_{5,N} = 499.24505256207357\), and \(\lambda_{6,N} = 502.3049699107303\).
    }
    \label{fig:eigenvalues}
\end{figure}


% Evolution of the error estimator \(\eta := \eta(u)\) as a function of the number of degrees of freedom.
\begin{figure}[htbp]
    \centering
    \begin{tikzpicture}
        \begin{loglogaxis}[
            xlabel={Number of degrees of freedom},
            ylabel={Error estimator},
            forget plot style={black!75, dotted},
            legend entries={\(\eta\)},
            legend pos=south west,
            cycle list name=cblist,
            grid=major,
        ]
        \addplot+ table[x=Ndofs, y={Eta}, col sep=comma] {error_estimator_eigenvalues.csv};
        \addplot [forget plot] table [
            col sep=comma,
            x=Ndofs,
            y={create col/linear regression={
                y=Eta
                % variance list={1000,800,600,500,400,200,100}
                }
            }] {error_estimator_eigenvalues.csv}
            coordinate [pos=0.8] (A)
            coordinate [pos=0.9] (B);
        \xdef\slope{\pgfplotstableregressiona}
        \draw (A) -| (B) node[pos=0.75, anchor=west] {\(\pgfmathprintnumber{\slope}\)};
        \end{loglogaxis}
    \end{tikzpicture}
    \caption{Error estimator \(\eta := \eta(u)\) as a function of the number of degrees of freedom, under eigenvalue-driven refinement.}
    \label{fig:error}
\end{figure}

% Monitoring the behavior of the error estimator \(\eta_i := \eta(\psi_i, \lambda_i)\) as a function of the number of degrees of freedom.
\begin{figure}[htbp]
    \centering
    \begin{tikzpicture}
        \begin{loglogaxis}[
            xlabel={Number of degrees of freedom},
            ylabel={Error estimator},
            legend entries={{\(\eta_0\)},
                            {\(\eta_1\)},
                            {\(\eta_2\)},
                            {\(\eta_3\)},
                            {\(\eta_4\)},
                            {\(\eta_5\)},
                            {\(\eta_6\)}},
            legend pos=south west,
            cycle list name=cblist,
            grid=major,
        ]
        % \addplot table[x=Ndofs, y={Eta}, col sep=comma] {error_estimator.csv};
        \addplot+ table[x=Ndofs, y=Eta Eigenvalue 0, col sep=comma] {error_estimator_eigenvalues.csv};
        \addplot+ table[x=Ndofs, y=Eta Eigenvalue 1, col sep=comma] {error_estimator_eigenvalues.csv};
        \addplot+ table[x=Ndofs, y=Eta Eigenvalue 2, col sep=comma] {error_estimator_eigenvalues.csv};
        \addplot+ table[x=Ndofs, y=Eta Eigenvalue 3, col sep=comma] {error_estimator_eigenvalues.csv};
        \addplot+ table[x=Ndofs, y=Eta Eigenvalue 4, col sep=comma] {error_estimator_eigenvalues.csv};
        \addplot+ table[x=Ndofs, y=Eta Eigenvalue 5, col sep=comma] {error_estimator_eigenvalues.csv};
        \addplot+ table[x=Ndofs, y=Eta Eigenvalue 6, col sep=comma] {error_estimator_eigenvalues.csv};

        \addplot [forget plot] table[
            x=Ndofs,
            y={create col/linear regression={y=Eta Eigenvalue 0}},
            col sep=comma] {error_estimator_eigenvalues.csv}
            coordinate [pos=0.8] (A)
            coordinate [pos=0.9] (B);
        \xdef\slope{\pgfplotstableregressiona}
        \draw (A) -| (B) node[pos=0.75, anchor=west] {\(\pgfmathprintnumber{\slope}\)};

        \end{loglogaxis}
    \end{tikzpicture}
    \caption{Error estimator \(\eta_i := \eta(\psi_i, \lambda_i)\) as a function of the number of degrees of freedom, under eigenvalue-driven refinement.}
    \label{fig:error}
\end{figure}


% Visualizing the error of the eigenvalues, after landscape-driven refinement.
\begin{figure}[htbp]
    \centering
    \begin{tikzpicture}
        \begin{loglogaxis}[
            xlabel={Number of degrees of freedom},
            ylabel={\(\left\lvert \lambda_{i,n} - \lambda_{i,N} \right\rvert\)},
            cycle list name=cblist,
            legend pos=south west,
            grid=major,
            % xmin=150, xmax=257000,
            legend entries={\(\operatorname{err}_0\),
                            \(\operatorname{err}_1\),
                            \(\operatorname{err}_2\),
                            \(\operatorname{err}_3\),
                            \(\operatorname{err}_4\),
                            \(\operatorname{err}_5\),
                            \(\operatorname{err}_6\)},
        ]
        \addplot+ table[x=Ndofs, y=Error Eigenvalue 0, col sep=comma] {error_eigenvalues_eigenvalues.csv};
        \addplot+ table[x=Ndofs, y=Error Eigenvalue 1, col sep=comma] {error_eigenvalues_eigenvalues.csv};
        \addplot+ table[x=Ndofs, y=Error Eigenvalue 2, col sep=comma] {error_eigenvalues_eigenvalues.csv};
        \addplot+ table[x=Ndofs, y=Error Eigenvalue 3, col sep=comma] {error_eigenvalues_eigenvalues.csv};
        \addplot+ table[x=Ndofs, y=Error Eigenvalue 4, col sep=comma] {error_eigenvalues_eigenvalues.csv};
        \addplot+ table[x=Ndofs, y=Error Eigenvalue 5, col sep=comma] {error_eigenvalues_eigenvalues.csv};
        \addplot+ table[x=Ndofs, y=Error Eigenvalue 6, col sep=comma] {error_eigenvalues_eigenvalues.csv};

        \addplot [forget plot] table[
            x=Ndofs,
            y={create col/linear regression={y=Error
            Eigenvalue 0}},
            col sep=comma] {error_eigenvalues_eigenvalues.csv}
            coordinate [pos=0.8] (A)
            coordinate [pos=0.9] (B);
        \xdef\slope{\pgfplotstableregressiona}
        \draw (A) -| (B) node[pos=0.75, anchor=west] {\(\pgfmathprintnumber{\slope}\)};
        \end{loglogaxis}
    \end{tikzpicture}
    \caption{Error \(\operatorname{err}_i := \left\lvert \lambda_{i,n} - \lambda_{i,N} \right\rvert\) as a function of the number of degrees of freedom, under eigenvalue-driven refinement.}
    \label{fig:error_alt}
\end{figure}


% Table of convergence ratios, under landscape-driven refinement.
% TODO: Fix scientific notation!
% \begin{table}[h!]
%     \begin{center}
%         \label{tab:convergence}
%         \pgfplotstabletypeset[
%             col sep=comma,
%             columns={Ndofs, Ratio 0, Ratio 1, Ratio 2, Ratio 3, Ratio 4, Ratio 5, Ratio 6},
%             columns/Ndofs/.style={
%                 column name={\#\textsc{DoFs}},
%                 int detect,
%                 column type={c}
%             },
%             columns/Ratio 0/.style={
%                 column name={\(\operatorname{ord}_0\)},
%                 precision=2,
%                 fixed zerofill,
%                 column type={c}
%             },
%             columns/Ratio 1/.style={
%                 column name={\(\operatorname{ord}_1\)},
%                 precision=2,
%                 fixed zerofill,
%                 column type={c}
%             },
%             columns/Ratio 2/.style={
%                 column name={\(\operatorname{ord}_2\)},
%                 precision=2,
%                 fixed zerofill,
%                 column type={c}
%             },
%             columns/Ratio 3/.style={
%                 column name={\(\operatorname{ord}_3\)},
%                 precision=2,
%                 fixed zerofill,
%                 column type={c}
%             },
%             columns/Ratio 4/.style={
%                 column name={\(\operatorname{ord}_4\)},
%                 precision=2,
%                 fixed zerofill,
%                 column type={c}
%             },
%             columns/Ratio 5/.style={
%                 column name={\(\operatorname{ord}_5\)},
%                 precision=2,
%                 fixed zerofill,
%                 column type={c}
%             },
%             columns/Ratio 6/.style={
%                 column name={\(\operatorname{ord}_6\)},
%                 precision=2,
%                 fixed zerofill,
%                 column type={c}
%             },
%             ]{ratio_ev_landscape.csv}
%             ]{ratio_ev_eigenvalues.csv}
%         \caption{
%             Order of convergence of the eigenvalues.
%             Here, \(\operatorname{err}_i \approx C_i N(h)^{-\operatorname{ord}_i}\).
%         }
%     \end{center}
% \end{table}

% Table of convergence (decay) ratio for the landscape error estimator.
% \begin{table}
%     \begin{center}
%         \label{tab:error}
%         \pgfplotstabletypeset[
%             col sep=comma,
%             columns={Ndofs, Ratio},
%             columns/Ndofs/.style={
%                 column name={\#\textsc{DoFs}},
%                 int detect,
%                 column type={c}
%             },
%             columns/Ratio/.style={
%                 column name={\(\operatorname{ord}\)},
%                 precision=2,
%                 fixed zerofill,
%                 column type={c}
%             },
%             ]{ratio_ev_eigenvalues.csv}
%         \caption{
%             Order of convergence of the landscape error estimator.
%             Here, \(\eta \approx C N(h)^{-\operatorname{ord}}\).
%         }
%     \end{center}
% \end{table}

% Pictures of the mesh during the adaptive algorithm, under landscape-driven refinement.
\begin{figure}[htbp]
    \begin{subfigure}[t]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{mesh/mesh_eigenvalues_0.png}
        \caption{Initial mesh.}
        \label{fig:mesh_0}
    \end{subfigure}
    \begin{subfigure}[t]{0.49\textwidth}
        \centering
        % \includegraphics[width=0.5\textwidth, trim={30cm 60cm 30cm 60cm}, clip]{mesh_10.png}
        \includegraphics[width=\textwidth]{mesh/mesh_eigenvalues_10.png}
        \caption{Mesh after ten iterations.}
        \label{fig:mesh_10}
    \end{subfigure}
    \vspace{1cm}
    \begin{subfigure}[t]{0.49\textwidth}
        \centering
        % \includegraphics[width=0.5\textwidth, trim={30cm 60cm 30cm 60cm}, clip]{mesh_20.png}
        \includegraphics[width=\textwidth]{mesh/mesh_eigenvalues_20.png}
        \caption{Mesh after twenty iterations.}
        \label{fig:mesh_20}
    \end{subfigure}
    \begin{subfigure}[t]{0.49\textwidth}
        \centering
        % \includegraphics[width=0.5\textwidth, trim={30cm 60cm 30cm 60cm}, clip]{mesh_30.png}
        \includegraphics[width=\textwidth]{mesh/mesh_eigenvalues_30.png}
        \caption{Mesh after thirty iterations.}
        \label{fig:mesh_30}
    \end{subfigure}
    \vspace{1cm}
    \begin{subfigure}[t]{0.49\textwidth}
        \centering
        % \includegraphics[width=0.5\textwidth, trim={30cm 60cm 30cm 60cm}, clip]{mesh_40.png}
        \includegraphics[width=\textwidth]{mesh/mesh_eigenvalues_40.png}
        \caption{Mesh after forty iterations.}
        \label{fig:mesh_40}
    \end{subfigure}
    \begin{subfigure}[t]{0.49\textwidth}
        \centering
        % \includegraphics[width=0.5\textwidth, trim={30cm 60cm 30cm 60cm}, clip]{mesh_49.png}
        \includegraphics[width=\textwidth]{mesh/mesh_eigenvalues_44.png}
        \caption{Final mesh.}
        \label{fig:mesh_50}
    \end{subfigure}
    \caption{Meshes during the adaptive algorithm, under eigenvalue-driven refinement.}
\end{figure}


% Eigenvalues, after landscape-driven refinement. Real part.
\begin{figure}[htbp]
    \begin{subfigure}[t]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{eigenvalue/ev0}
        \caption{Eigenfunction \(\psi_0\).}
        \label{fig:eigenfunction_0}
    \end{subfigure}
    \begin{subfigure}[t]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{eigenvalue/ev1}
        \caption{Eigenfunction \(\psi_1\).}
        \label{fig:eigenfunction_1}
    \end{subfigure}
    \vspace{1cm}
    \begin{subfigure}[t]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{eigenvalue/ev2}
        \caption{Eigenfunction \(\psi_2\).}
        \label{fig:eigenfunction_2}
    \end{subfigure}
    \begin{subfigure}[t]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{eigenvalue/ev3}
        \caption{Eigenfunction \(\psi_3\).}
        \label{fig:eigenfunction_3}
    \end{subfigure}
    \vspace{1cm}
    \begin{subfigure}[t]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{eigenvalue/ev4}
        \caption{Eigenfunction \(\psi_4\).}
        \label{fig:eigenfunction_4}
    \end{subfigure}
    \begin{subfigure}[t]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{eigenvalue/ev5}
        \caption{Eigenfunction \(\psi_5\).}
        \label{fig:eigenfunction_5}
    \end{subfigure}
    \vspace{1cm}
    \begin{subfigure}[t]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{eigenvalue/ev6}
        \caption{Eigenfunction \(\psi_6\).}
        \label{fig:eigenfunction_6}
    \end{subfigure}
    \caption{(Real part of the) eigenfunctions \(\psi_i\) for \(i = 0, \ldots, 6\) at the final iteration of the adaptive algorithm, under eigenvalue-driven refinement.}
\end{figure}


% Eigenvalues, after eigenvalue-driven refinement. Absolute value.
\begin{figure}[htbp]
    \begin{subfigure}[t]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{eigenvalue/abs_ev0}
        \caption{Eigenfunction \(\psi_0\).}
        \label{fig:eigenfunction_0}
    \end{subfigure}
    \begin{subfigure}[t]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{eigenvalue/abs_ev1}
        \caption{Eigenfunction \(\psi_1\).}
        \label{fig:eigenfunction_1}
    \end{subfigure}
    \vspace{1cm}
    \begin{subfigure}[t]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{eigenvalue/abs_ev2}
        \caption{Eigenfunction \(\psi_2\).}
        \label{fig:eigenfunction_2}
    \end{subfigure}
    \begin{subfigure}[t]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{eigenvalue/abs_ev3}
        \caption{Eigenfunction \(\psi_3\).}
        \label{fig:eigenfunction_3}
    \end{subfigure}
    \vspace{1cm}
    \begin{subfigure}[t]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{eigenvalue/abs_ev4}
        \caption{Eigenfunction \(\psi_4\).}
        \label{fig:eigenfunction_4}
    \end{subfigure}
    \begin{subfigure}[t]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{eigenvalue/abs_ev5}
        \caption{Eigenfunction \(\psi_5\).}
        \label{fig:eigenfunction_5}
    \end{subfigure}
    \vspace{1cm}
    \begin{subfigure}[t]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{eigenvalue/abs_ev6}
        \caption{Eigenfunction \(\psi_6\).}
        \label{fig:eigenfunction_6}
    \end{subfigure}
    \caption{Norm of the eigenfunctions \(\psi_i\) for \(i = 0, \ldots, 6\) at the final iteration of the adaptive algorithm, under eigenvalue-driven refinement.}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{``Universality'' of the landscape-function-driven adaptive algorithm}

We will now describe the ``universality'' of the landscape-function-driven adaptive algorithm.
Assume we have convergence of the approximated landscape functions to the true landscape function, i.e.,
\begin{equation}
    \label{eq:convergence}
    \lVert u - u_h \rVert_{V} \to 0 \text{ as } h \to 0.
\end{equation}
We want to show convergence of the eigenvalue approximations to the true eigenvalues, i.e.,
\begin{equation}
    \label{eq:eigenvalue_convergence}
    \lvert \lambda - \lambda_h \rvert \to 0 \text{ as } h \to 0.
\end{equation}
and convergence of the eigenfunction approximations to the true eigenfunctions, i.e.,
\begin{equation}
    \label{eq:eigenfunction_convergence}
    \lVert \psi - \psi_h \rVert_{V} \to 0 \text{ as } h \to 0.
\end{equation}

% Due to the stability of the finite element method, we have quasi-optimality, i.e.,
% \begin{equation}
%     \label{eq:quasi_optimality}
%     \inf_{v_h \in V_h} \lVert u - v_h \rVert_{V} \leq \lVert u - u_h \rVert_{V} \lesssim \inf_{v_h \in V_h} \lVert u - v_h \rVert_{V}.
% \end{equation}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Mass concentration}

Assume that we have \(L^2\)-convergence of the landscape functions, i.e., \(\lVert u - u_h \rVert_{L^2} \to 0\) as \(h \to 0\).
Using Besel-Parseval, we have that all the Fourier coefficients of the difference \(u - u_h\) go to zero.
We describe these coefficients:
\begin{align*}
    \label{eq:fourier}
    (u - u_h, \psi_n) & = (u, \psi_n) - (u_h, \psi_n)\\
    & = \left(1, \frac{\psi_n}{\lambda_n}\right) -
    \left(\sum_{i=1}^{N(h)} (u_h, \psi_{h, i}) \psi_{h, i}, \psi_n\right) \\
    & = \left(1, \frac{\psi_n}{\lambda_n}\right) - \sum_{i=1}^{N(h)}  \left(1, \frac{\psi_{h, i}}{\lambda_{h, i}} \right) (\psi_{h, i}, \psi_n) \\
    & = \left(1, \frac{\psi_n}{\lambda_n}\right) - \left(1, \sum_{i=1}^{N(h)} \frac{\psi_{h, i}}{\lambda_{h, i}} (\psi_{h, i}, \psi_n)\right)
\end{align*}
We have the following bound:
\begin{equation}
    \label{eq:fourier_bound}
    \left\lvert
    \left(1, \frac{\psi_n}{\lambda_n}\right) - \left(1, \sum_{i=1}^{N(h)} \frac{\psi_{h, i}}{\lambda_{h, i}} (\psi_{h, i}, \psi_n)\right)
    \right\rvert
\leq \left\lVert u - u_h \right\rVert_{L^2},
\end{equation}
for all \(n \in \mathbb{N}\).
This can be interpreted as a concentration of the mass of the eigenfunctions upon convergence of the discrete landscape function to the true landscape function.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Weak convergence and best approximation techniques}

The later suggests some sequence of functions that may have better convergence properties.

Let \(w \in L^2(\Omega)\) be a test function.
Define \(\hat \psi_{h, n} := \sum_{i=1}^{N(h)} \frac{(\psi_n, \psi_{h, i})}{\lambda_{h, i}} \psi_{h, i}\), for all \(n \in \mathbb{N}\), and \(h > 0\).
Denote the true (accumulated) eigenspace by \( E_n := \text{span}\{\psi_1, \ldots, \psi_n\}\) for all \(n \in \mathbb{N}\).
Analogously, define its discete counterpart by \(E_{h, n} := \text{span}\{\psi_{h, 1}, \ldots, \psi_{h, n}\}\) for all \(n = 1, \dots, N(h)\) and \(h > 0\).
Define the \(L^2\)-projection operator \(P_n: L^2(\Omega) \to E_n\) and its discrete counterpart \(P_{h, n}: L^2(\Omega) \to E_{h, n}\).
Notice that \(\hat \psi_{h, n} \in E_{h, N(h)}\).

\begin{align*}
    % \label{eq:projection}
    \left\lVert \frac{\psi_n}{\lambda_n} - P_{h, N(h)}\left( \frac{\psi_n}{\lambda_n} \right) \right\rVert_{L^2}^2
    & \leq \left\lVert \frac{\psi_n}{\lambda_n} - \hat \psi_{h, n} \right\rVert_{L^2}^2 \\
    & = \left\lVert \sum_{i=1}^{N(h)} \left( \frac{(\psi_n, \psi_{h, i})}{\lambda_n} - \frac{(\psi_n, \psi_{h, i})}{\lambda_{h, i}} \right) \psi_{h, i} \right\rVert_{L^2}^2 \\
    & = \sum_{i=1}^{N(h)} \lvert (\psi_n, \psi_{h, i}) \rvert^2 \left\lvert \frac{1}{\lambda_n} - \frac{1}{\lambda_{h, i}} \right\rvert^2 \\
    & \leq \max_{i=1, \dots, N(h)} \left\lvert \frac{1}{\lambda_n} - \frac{1}{\lambda_{h, i}} \right\rvert^2.
\end{align*}

% \begin{align*}
%     % \label{eq:projection}
%     \left\lVert \frac{\psi_n}{\lambda_n} - \hat \psi_{h, n} \right\rVert_{L^2}^2
%     & = \left\lVert \sum_{i=1}^{N(h)} \left( \frac{(\psi_n, \psi_{h, i})}{\lambda_n} - \frac{(\psi_n, \psi_{h, i})}{\lambda_{h, i}} \right) \psi_{h, i} \right\rVert_{L^2}^2 \\
%     & = \sum_{i=1}^{N(h)} \lvert (\psi_n, \psi_{h, i}) \rvert^2 \left\lvert \frac{1}{\lambda_n} - \frac{1}{\lambda_{h, i}} \right\rvert^2 \\
%     & \leq \max_{i=1, \dots, N(h)} \left\lvert \frac{1}{\lambda_n} - \frac{1}{\lambda_{h, i}} \right\rvert^2.
% \end{align*}

% Rewriting the above expression, we have
% \begin{equation}
%     \label{eq:projection-bound}
%     \left\lVert \psi_n - P_{h, n}(\psi_n) \right\rVert_{L^2}
%     \leq \max_{i=1, \dots, N(h)} \left\lvert 1 - \frac{\lambda_n}{\lambda_{h, i}} \right\rvert.
% \end{equation}

We can repeat a similar argument, where \(N = N(h)\), to show that
\begin{align*}
    % \label{eq:projection}
    \left\lVert \frac{\psi_N}{\lambda_N} - \frac{(\psi_N, \psi_{h, N})}{\lambda_{h, N}} \psi_{h, N} \right\rVert_{L^2}^2
    & = \left\lVert P_{h, N-1}\left( \frac{\psi_N}{\lambda_N} \right)\right\rVert_{L^2}^2
    +  \left\lvert (\psi_N, \psi_{h, N}) \right\rvert^2 \left\lvert \frac{1}{\lambda_N} - \frac{1}{\lambda_{h, N}} \right\rvert^2 \\
    & \leq \left\lVert P_{h, N-1}\left( \frac{\psi_N}{\lambda_N} \right)\right\rVert_{L^2}^2
    +  \left\lvert \frac{1}{\lambda_N} - \frac{1}{\lambda_{h, N}} \right\rvert^2.
\end{align*}

\end{document}
